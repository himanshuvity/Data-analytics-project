{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f314d21-369d-4399-94e9-e99c5064af09",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StandardScaler, MinMaxScaler\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimpute\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SimpleImputer\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m stats\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from scipy import stats\n",
    "import dask.dataframe as dd\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(filename='data_processing.log', level=logging.INFO)\n",
    "\n",
    "# Load Data\n",
    "def load_data(file_path, file_type='csv'):\n",
    "    try:\n",
    "        if file_type == 'csv':\n",
    "            return pd.read_csv(file_path)\n",
    "        elif file_type == 'excel':\n",
    "            return pd.read_excel(file_path)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported file type\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error loading data: {e}\")\n",
    "        return None\n",
    "\n",
    "# Outlier Detection and Handling\n",
    "def handle_outliers(df, method='z_score', threshold=3):\n",
    "    try:\n",
    "        if method == 'z_score':\n",
    "            z_scores = np.abs(stats.zscore(df.select_dtypes(include=np.number)))\n",
    "            return df[(z_scores < threshold).all(axis=1)]\n",
    "        elif method == 'iqr':\n",
    "            Q1 = df.quantile(0.25)\n",
    "            Q3 = df.quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            return df[~((df < (Q1 - 1.5 * IQR)) | (df > (Q3 + 1.5 * IQR))).any(axis=1)]\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported outlier detection method\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error handling outliers: {e}\")\n",
    "        return df\n",
    "\n",
    "# Missing Value Imputation\n",
    "def impute_missing_values(df, strategy='mean'):\n",
    "    try:\n",
    "        imputer = SimpleImputer(strategy=strategy)\n",
    "        df_imputed = pd.DataFrame(imputer.fit_transform(df), columns=df.columns)\n",
    "        return df_imputed\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error imputing missing values: {e}\")\n",
    "        return df\n",
    "\n",
    "# Normalization\n",
    "def normalize_data(df, method='min_max'):\n",
    "    try:\n",
    "        if method == 'min_max':\n",
    "            scaler = MinMaxScaler()\n",
    "        elif method == 'z_score':\n",
    "            scaler = StandardScaler()\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported normalization method\")\n",
    "        df_scaled = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\n",
    "        return df_scaled\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error normalizing data: {e}\")\n",
    "        return df\n",
    "\n",
    "# Data Processing Pipeline\n",
    "def process_data(file_path, file_type='csv'):\n",
    "    try:\n",
    "        df = load_data(file_path, file_type)\n",
    "        if df is not None:\n",
    "            df = handle_outliers(df)\n",
    "            df = impute_missing_values(df)\n",
    "            df = normalize_data(df)\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing data: {e}\")\n",
    "        return None\n",
    "\n",
    "# Batch Processing with Dask\n",
    "def batch_process_data(file_paths, file_type='csv'):\n",
    "    try:\n",
    "        dfs = [dd.from_pandas(process_data(fp, file_type), npartitions=1) for fp in file_paths]\n",
    "        ddf = dd.concat(dfs)\n",
    "        return ddf.compute()\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in batch processing: {e}\")\n",
    "        return None\n",
    "\n",
    "# Data Validation\n",
    "def validate_data(df):\n",
    "    try:\n",
    "        # Example validation rule: Check for negative values in numeric columns\n",
    "        if (df.select_dtypes(include=np.number) < 0).any().any():\n",
    "            logging.warning(\"Data contains negative values.\")\n",
    "        # Add more validation rules as needed\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error validating data: {e}\")\n",
    "\n",
    "# Data Visualization\n",
    "def visualize_data(df):\n",
    "    try:\n",
    "        sns.pairplot(df)\n",
    "        plt.show()\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error visualizing data: {e}\")\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    file_paths = ['data1.csv', 'data2.csv']\n",
    "    processed_data = batch_process_data(file_paths)\n",
    "\n",
    "    if processed_data is not None:\n",
    "        validate_data(processed_data)\n",
    "        visualize_data(processed_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d01a59-d0b2-4643-bd0b-e87aea734b6b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
